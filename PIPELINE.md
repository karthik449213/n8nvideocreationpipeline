# Pipeline Architecture and Design

This document explains the components of the automated video creation pipeline and how they work together.

## 1. Overview

The entire process is orchestrated by **n8n**, an open‑source workflow automation tool that acts as our agentic platform. Each major stage in the pipeline is a node or a sub‑workflow; complex logic is implemented in Node.js scripts under `scripts/`.

The pipeline can be triggered in two ways: a **cron schedule** (e.g. daily at midnight) or an **HTTP webhook** for on‑demand execution. Once started, the flow never requires human input.

## 2. Stages

1. **Idea shortlist**
   - A GPT model (`idea-generator.js`) is asked for 10 video concepts in the "zack d films" style.
   - The results are rated internally (score via secondary prompt) and the top idea is selected. Optionally, we can store the full set in a database (e.g. SQLite, Google Sheets) for reuse.

2. **Prompt generation**
   - The winning idea is sent to another GPT endpoint (`prompt-generator.js`) which returns:
     - A set of 5–8 image prompts (3D rendered stills, cinematic angles).
     - A video‑style prompt to feed into an animation/3D service or to guide the `ffmpeg` assembler.

3. **Asset creation**
   - Use an image generation API (Stable Diffusion, DALL·E, Midjourney CLI) to produce high‑resolution frames/images.
   - Each prompt is converted into an image, stored in cloud storage (Google Cloud Storage, AWS S3, or local filesystem). The URLs/paths are passed forward.
   - Optionally, we may request multiple versions and automatically pick the best using CLIP scoring.

4. **Script & narration**
   - GPT writes a short script based on the chosen idea and images. This may include camera directions to assist the assembler.
   - The text is converted to speech using a TTS API (e.g. ElevenLabs, Google Cloud TTS). The audio file(s) are stored alongside the images.

5. **Video assembly**
   - A Node script (`video-assembler.js`) takes the images and audio and constructs the final MP4 using `ffmpeg`. Transition effects, pan/zoom (Ken Burns), and simple 3D motion are encoded here.
   - If a dedicated 3D engine is required (e.g., Blender), the script can call Blender headlessly with a generated Python script.

6. **Publishing**
   - The completed video is uploaded to YouTube via the Google APIs library (`publish-to-youtube.js`). Metadata such as title, description, tags are generated by GPT.
   - The workflow writes the published video ID to a log or Google Sheet for tracking.

7. **Cleanup / Notification**
   - Temporary files are removed.
   - A webhook or email is sent to signal completion.

## 3. Extensibility

- **Additional content types**: Add nodes for 2D animation, voice cloning, subtitles, etc.
- **Quality control**: Insert a verification step that runs automated checks (audio loudness, aspect ratio, safe‑search).
- **Parallelism**: n8n handles concurrent executions; each run is independent.

## 4. Notes on running headless

All external tools (e.g., `ffmpeg`, Blender) must be installed on the machine running n8n. Credentials are provided through environment variables and n8n's credential manager.

## 5. Example n8n flow

See `n8n/three_d_pipeline.json` for a complete exported flow. Import it into your n8n instance to get started.